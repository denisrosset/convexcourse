{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:\n",
    "\n",
    "- [NLopt](https://github.com/JuliaOpt/NLopt.jl) Powell's derivative-free algorithms\n",
    "- [Optim](https://github.com/JuliaNLSolvers/Optim.jl) Gradient-based algorithms, autodifferentiation support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLopt\n",
    "using Optim\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear unconstrained optimization\n",
    "\n",
    "Example: projective measurements on the singlet state $\\left| \\Psi^- \\right> = \\left| 01 - 10 \\right>/\\sqrt{2} \\in \\mathcal{H}_\\text{A} \\otimes (\\mathcal{H}_\\text{B})$.\n",
    "\n",
    "A for Alice and B for Bob.\n",
    "\n",
    "We parameterize local measurements by qubit states.\n",
    "\n",
    "$$ \\left| \\vec{\\phi} \\right> = \\begin{pmatrix} \\cos \\phi_1 \\\\ e^{i \\phi_2} \\sin \\phi_1 \\end{pmatrix}, \\qquad \\left| \\vec{\\phi} \\right>_\\perp = \\begin{pmatrix} -\\sin \\phi_1 \\\\ e^{i \\phi_2} \\cos \\phi_1 \\end{pmatrix}  $$\n",
    "\n",
    "There are two measurement settings indexed by $s=1,2$ on $\\mathcal{H}_\\text{A}$, corresponding to two projectors $\\{\\left| \\alpha_1 \\right>, \\left| \\alpha_1 \\right>_\\perp\\}$ and $\\{\\left| \\alpha_2 \\right>, \\left| \\alpha_2 \\right>_\\perp \\}$, and two measurement settings indexed by $t=1,2$ on $\\mathcal{H}_\\text{B}$, corresponding to $\\{\\left| \\beta_1 \\right>, \\left| \\beta_1 \\right>_\\perp\\}$ and $\\{\\left| \\beta_2 \\right>, \\left| \\beta_2 \\right>_\\perp\\}$.\n",
    "\n",
    "The measurement outcomes are 1-based (Julia, Matlab), so that $a=1$ corresponds to $\\left | \\alpha_s \\right >$ and $a=2$ to $\\left | \\alpha_s \\right >_\\perp$. The same for Bob: for $b=1$ corresponds to $\\left | \\beta_t \\right>$ and $b=2$ to $\\left| \\beta_t \\right>_\\perp$\n",
    "\n",
    "Then we consider the joint, conditional probability distribution $$ P_\\text{AB|ST}(a,b|s,t) $$. We compute using Born's rule:\n",
    "\n",
    "$$ P_\\text{AB|ST}(1,1|s,t) = \\left( \\left< \\alpha_s \\right | \\otimes \\left< \\beta_t \\right | \\right ) \\left | \\Psi^- \\right > $$\n",
    "$$ P_\\text{AB|ST}(2,1|s,t) = \\left( \\left< \\alpha_s \\right |_\\perp \\otimes \\left< \\beta_t \\right | \\right ) \\left | \\Psi^- \\right > $$\n",
    "$$ P_\\text{AB|ST}(1,2|s,t) = \\left( \\left< \\alpha_s \\right | \\otimes \\left< \\beta_t \\right |_\\perp \\right ) \\left | \\Psi^- \\right > $$\n",
    "$$ P_\\text{AB|ST}(2,2|s,t) = \\left( \\left< \\alpha_s \\right |_\\perp \\otimes \\left< \\beta_t \\right |_\\perp \\right ) \\left | \\Psi^- \\right > $$\n",
    "\n",
    "We want to maximize the CHSH expression:\n",
    "\n",
    "$$ C = \\sum_{abst} (-1)^{(a-1)+(b-1)+(s-1)(t-1)} P_\\text{AB|ST}(a,b|s,t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singlet_proj_prob (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function singlet_proj_prob(x)\n",
    "# Computes the joint probability distribution given by two projective measurements on each subsystem of a singlet state\n",
    "    P = zeros(eltype(x)::Type, (2, 2, 2, 2));\n",
    "    A = zeros(Complex{eltype(x)::Type}, (2, 2, 2));\n",
    "    B = zeros(Complex{eltype(x)::Type}, (2, 2, 2));\n",
    "    A[:,1,1] = [ cos(x[1])\n",
    "                 sin(x[1])];\n",
    "    A[:,2,1] = [-sin(x[1])\n",
    "                 cos(x[1])];\n",
    "    A[:,1,2] = [ cos(x[2])\n",
    "                 sin(x[2])];\n",
    "    A[:,2,2] = [-sin(x[2])\n",
    "                 cos(x[2])];\n",
    "    B[:,1,1] = [ cos(x[3])\n",
    "                 sin(x[3])];\n",
    "    B[:,2,1] = [-sin(x[3])\n",
    "                 cos(x[3])];\n",
    "    B[:,1,2] = [ cos(x[4])\n",
    "                 sin(x[4])];\n",
    "    B[:,2,2] = [-sin(x[4])\n",
    "                 cos(x[4])];\n",
    "    for s = 1:2\n",
    "        for t = 1:2\n",
    "            for a = 1:2\n",
    "                for b = 1:2\n",
    "                    ov = kron(A[:,a,s], B[:,b,t])' * [0; 1; -1; 0]/sqrt(2);\n",
    "                    P[a,b,s,t] = real(conj(ov)*ov);\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return P\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chsh (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function chsh(P)\n",
    "# Computes the CHSH expression from a joint probability distribution\n",
    "    C = 0;\n",
    "    for s = 1:2\n",
    "        for t = 1:2\n",
    "            for a = 1:2\n",
    "                for b = 1:2\n",
    "                    C = C + (-1)^((a-1)+(b-1)+(s-1)*(t-1)) * P[a,b,s,t];\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim\n",
    "# Set up objective function, minus sign for maximization\n",
    "f(x) = -chsh(singlet_proj_prob(x))\n",
    "# Random initial point\n",
    "x0 = rand(Float64, 4)*2*pi\n",
    "verbose = true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value    √(Σ(yᵢ-ȳ)²)/n \n",
      "------   --------------    --------------\n",
      "     0    -1.797219e+00     9.927339e-01\n",
      " * time: 0.00016021728515625\n",
      "    10    -2.191993e+00     2.288082e-01\n",
      " * time: 0.002167224884033203\n",
      "    20    -2.745744e+00     6.492282e-02\n",
      " * time: 0.0038650035858154297\n",
      "    30    -2.812342e+00     4.326778e-03\n",
      " * time: 0.005515098571777344\n",
      "    40    -2.826882e+00     1.381557e-03\n",
      " * time: 0.006715059280395508\n",
      "    50    -2.828178e+00     8.475774e-05\n",
      " * time: 0.0071620941162109375\n",
      "    60    -2.828390e+00     3.208715e-05\n",
      " * time: 0.007607221603393555\n",
      "    70    -2.828419e+00     4.963490e-06\n",
      " * time: 0.008007049560546875\n",
      "    80    -2.828425e+00     7.722723e-07\n",
      " * time: 0.00844717025756836\n",
      "    90    -2.828427e+00     7.894150e-08\n",
      " * time: 0.008884191513061523\n",
      "   100    -2.828427e+00     4.873629e-08\n",
      " * time: 0.00929403305053711\n",
      "\n",
      "\n",
      "Maximium: 2.828427 in 103 iterations\n",
      "Diff: 2.9E-09\n"
     ]
    }
   ],
   "source": [
    "# Derivative-free, and possibly not differentiable: Nelder-Mead recommended\n",
    "res = Optim.optimize(f, x0, Optim.NelderMead(), Optim.Options(f_tol = 1e-8, show_trace = verbose, show_every = 10))\n",
    "@printf(\"\\n\\nMaximum: %f in %d iterations\\n\", -Optim.minimum(res), Optim.iterations(res))\n",
    "@printf(\"Diff: %0.1E\\n\", abs(Optim.minimum(res) + 2*sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maximum: 2.828427 in 60 iterations\n",
      "Diff: 3.3E-07\n"
     ]
    }
   ],
   "source": [
    "# Derivative-free but differentiable: Powell's BOBYQA building a quadratic model\n",
    "\n",
    "# Uses NLopt, so different calling convention.\n",
    "# Warning: Powell's method becomes interesting for medium scale problems, here not much difference with Nelder-Mead\n",
    "\n",
    "function vis(x::Vector, grad::Vector)\n",
    "# we fake a \"grad\" argument to make NLopt happy\n",
    "   val = -chsh(singlet_proj_prob(x))\n",
    "   return val\n",
    "end\n",
    "\n",
    "opt = Opt(:LN_BOBYQA, 4)\n",
    "opt.lower_bounds = zeros(4)*1.0\n",
    "opt.upper_bounds = ones(4)*2*pi*1.0\n",
    "opt.min_objective = vis\n",
    "opt.ftol_rel = 1e-8\n",
    "(minf, minx, ret) = NLopt.optimize(opt, x0)\n",
    "@printf(\"\\n\\nMaximum: %f in %d iterations\\n\", -minf, opt.numevals)\n",
    "@printf(\"Diff: %0.1E\\n\", abs(maxf + 2*sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0    -7.078627e-02     3.782969e+00\n",
      " * time: 5.698204040527344e-5\n",
      "\n",
      "\n",
      "Maximium: 2.828427 in 5 iterations\n",
      "Diff: 4.4E-16\n"
     ]
    }
   ],
   "source": [
    "# Using information in provided Hessian, here using autodifferentiation\n",
    "res = Optim.optimize(f, x0, Optim.Newton(), Optim.Options(f_tol = 1e-8, show_trace = verbose, show_every = 10); autodiff = :forward)\n",
    "@printf(\"\\n\\nMaximum: %f in %d iterations\\n\", -Optim.minimum(res), Optim.iterations(res))\n",
    "@printf(\"Diff: %0.1E\\n\", abs(Optim.minimum(res) + 2*sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0    -7.078627e-02     3.782969e+00\n",
      " * time: 5.0067901611328125e-5\n",
      "    10    -2.828427e+00     2.889560e-07\n",
      " * time: 0.0009119510650634766\n",
      "\n",
      "\n",
      "Maximium: 2.828427 in 11 iterations\n",
      "Diff: 4.4E-16\n"
     ]
    }
   ],
   "source": [
    "# Only gradients provided: BSGS recommended (medium scale), Hessian is approximated, gradients from autodifferentation\n",
    "res = Optim.optimize(f, x0, Optim.BFGS(), Optim.Options(f_tol = 1e-8, show_trace = verbose, show_every = 10); autodiff = :forward)\n",
    "@printf(\"\\n\\nMaximum: %f in %d iterations\\n\", -Optim.minimum(res), Optim.iterations(res))\n",
    "@printf(\"Diff: %0.1E\\n\", abs(Optim.minimum(res) + 2*sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0    -7.078627e-02     3.782969e+00\n",
      " * time: 6.604194641113281e-5\n",
      "    10    -2.828427e+00     1.388879e-05\n",
      " * time: 0.002519845962524414\n",
      "\n",
      "\n",
      "Maximium: 2.828427 in 12 iterations\n",
      "Diff: 4.4E-16\n"
     ]
    }
   ],
   "source": [
    "# Gradients provided, large scale, use limited memory BFGS\n",
    "res = Optim.optimize(f, x0, Optim.LBFGS(), Optim.Options(show_trace = verbose, show_every = 10); autodiff = :forward)\n",
    "@printf(\"\\n\\nMaximum: %f in %d iterations\\n\", -Optim.minimum(res), Optim.iterations(res))\n",
    "@printf(\"Diff: %0.1E\\n\", abs(Optim.minimum(res) + 2*sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "\n",
    "- Find a \"nice\" algebraic expression for the maximizer corresponding to the objective value $2 \\sqrt{2}$\n",
    "- Add support for the complex part, see if there is any difference (real vs. complex quantum mechanics!).\n",
    "- Parameterize the quantum state as $\\left| \\gamma \\right> = \\cos \\gamma \\left| 01 \\right> + \\sin \\gamma \\left| 10 \\right>$. Which state leads to the largest violation?\n",
    "\n",
    "## Bonus open ended exercices\n",
    "\n",
    "- Plot the objective value for the iteration number; either use the trace, a callback, or just stop the algorithm after $n$ steps.\n",
    "- (Beware package incompatibilities) Parameterize $\\cos \\alpha, \\sin \\alpha$ as $x_1, x_2$ with $x_1^2 + x_2^2 = 1$ for each angle, and rewrite the above as a polynomial optimization problem. Solve with JuMP and the polynomial optimization extensions. Can you certify that $2 \\sqrt(2)$ is a global maximum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
